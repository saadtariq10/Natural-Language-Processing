{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ixIK_5Y1srMb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/final_movie_data.csv')"
      ],
      "metadata": {
        "id": "b8t_reFvtIb6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lower\n",
        "df['name'] = df['name'].str.lower()\n",
        "df['description'] = df['description'].str.lower()"
      ],
      "metadata": {
        "id": "WiLb6objtRWn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove html tags\n",
        "import re\n",
        "def remove_html_tags(text):\n",
        "    clean = re.compile('<.*?>')\n",
        "    return re.sub(clean, '', text)"
      ],
      "metadata": {
        "id": "FfbrP847wIq1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_html_tags(df['description'][0])\n",
        "re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CepA8LP_wxJQ",
        "outputId": "ab0948ff-3005-4345-e6bd-89f95f973d88"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 're' from '/usr/lib/python3.10/re.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'] = df['description'].fillna('').apply(remove_html_tags)"
      ],
      "metadata": {
        "id": "JPRVT8gmycLO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'] = df['description'].apply(remove_html_tags)\n"
      ],
      "metadata": {
        "id": "oyFjhWRhxuAO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove urls\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub('', text)"
      ],
      "metadata": {
        "id": "nzGQVN3sylLN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'] = df['description'].apply(remove_urls)"
      ],
      "metadata": {
        "id": "YwxRa0_yy3Kf"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punctuation\n",
        "import string\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "uwakHddYy6Tf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'] = df['description'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "Th0aqd7JzT4L"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spelling correction\n",
        "from textblob import TextBlob\n",
        "def correct_spelling(text):\n",
        "    return str(TextBlob(text).correct())"
      ],
      "metadata": {
        "id": "5aKcNQeFzedm"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['description'] = df['description'].apply(correct_spelling)"
      ],
      "metadata": {
        "id": "9ngvjdJozwi6"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming df is your DataFrame and correct_spelling is your function\n",
        "tqdm.pandas()\n",
        "\n",
        "#df['description'] = df['description'].progress_apply(correct_spelling)\n"
      ],
      "metadata": {
        "id": "wTo1rTN6z6fG"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stop words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z_voCZF2aR7",
        "outputId": "a202a377-961e-4d77-9895-7662732da09b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Optimized function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "df['description'] = df['description'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "EiVaaPQn2fSb"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spacy tokenization\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO_PjYNQ2oX1",
        "outputId": "4b65be26-807b-4af1-974d-065bd07010df"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_tokenizer(sentence):\n",
        "    mytokens = nlp(sentence)\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in string.punctuation ]\n",
        "    return mytokens\n",
        ""
      ],
      "metadata": {
        "id": "n9d-ld2B3vm6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_tokenizer(df['description'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRhQUa5I36eJ",
        "outputId": "6c00dfd3-91af-492a-f5bc-1411c2217b81"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imprison',\n",
              " '1940s',\n",
              " 'double',\n",
              " 'murder',\n",
              " 'wife',\n",
              " 'lover',\n",
              " 'upstande',\n",
              " 'banker',\n",
              " 'andy',\n",
              " 'dufresne',\n",
              " 'begin',\n",
              " 'new',\n",
              " 'life',\n",
              " 'shawshank',\n",
              " 'prison',\n",
              " 'put',\n",
              " 'accounting',\n",
              " 'skill',\n",
              " 'work',\n",
              " 'amoral',\n",
              " 'warden',\n",
              " 'long',\n",
              " 'stretch',\n",
              " 'prison',\n",
              " 'dufresne',\n",
              " 'come',\n",
              " 'admire',\n",
              " 'inmate',\n",
              " 'include',\n",
              " 'old',\n",
              " 'prisoner',\n",
              " 'name',\n",
              " 'red',\n",
              " 'integrity',\n",
              " 'unquenchable',\n",
              " 'sense',\n",
              " 'hope']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df['description'] = df['description'].apply(spacy_tokenizer)"
      ],
      "metadata": {
        "id": "HSgHN2Hs38FT"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhUYpbhH6_-0",
        "outputId": "ae5230d4-1034-4b4c-b3f4-ee6f3f209b08"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name                    0\n",
            "description             0\n",
            "original_description    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "df['description'] = df['description'].progress_apply(spacy_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnauWyRj3_Ks",
        "outputId": "1d81230a-2ab9-4a72-e44b-32531ee6579b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9522/9522 [01:27<00:00, 108.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Download the WordNet data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWvBCqyw4UiH",
        "outputId": "3488e301-1050-4165-bb25-f8ec2f41e5e6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to lemmatize text\n",
        "def lemmatize_text(text):\n",
        "    if isinstance(text, str):  # Check if the text is a string\n",
        "        # Tokenize the text\n",
        "        words = nltk.word_tokenize(text)\n",
        "        # Lemmatize each word\n",
        "        return ' '.join(wordnet_lemmatizer.lemmatize(word) for word in words)\n",
        "    else:\n",
        "        return text  # Return the input unchanged if it's not a string\n"
      ],
      "metadata": {
        "id": "5A8n9piI6Q_H"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply lemmatization to the 'description' column\n",
        "tqdm.pandas()  # Initialize tqdm for progress_apply\n",
        "df['description'] = df['description'].progress_apply(lemmatize_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfkdsSTx65Yr",
        "outputId": "3d7df7c6-50c1-452a-ae4d-53c77caf8c5f"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9522/9522 [00:00<00:00, 693623.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the lemmatization function with an example sentence\n",
        "sentence = \"The leaves on the trees are falling rapidly.\"\n",
        "lemmatized_sentence = lemmatize_text(sentence)"
      ],
      "metadata": {
        "id": "h8-L22d66-Ra"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original:\", sentence)\n",
        "print(\"Lemmatized:\", lemmatized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrpaC8hm8ejT",
        "outputId": "c1cd612d-457b-4f8d-84a6-5147cbaab55a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: The leaves on the trees are falling rapidly.\n",
            "Lemmatized: The leaf on the tree are falling rapidly .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WAJq7dBu9d5N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}